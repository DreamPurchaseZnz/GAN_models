{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import os, sys\n",
    "# Packages\n",
    "from model import build_graph, config\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from aae import Operation\n",
    "import dataset\n",
    "from process import Process\n",
    "import sampler\n",
    "import plot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(run_load_from_file=False):\n",
    "    # load MNIST images\n",
    "    images, labels = dataset.load_test_images()\n",
    "\n",
    "    # config\n",
    "    opt = Operation()\n",
    "    opt.check_dir(config.ckpt_dir, is_restart=False)\n",
    "    opt.check_dir(config.log_dir, is_restart=True)\n",
    "\n",
    "    max_epoch = 2010\n",
    "    num_trains_per_epoch = 500\n",
    "    batch_size_u = 100\n",
    "\n",
    "    # training\n",
    "    with tf.device(config.device):\n",
    "        h = build_graph()\n",
    "\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        '''\n",
    "         Load from checkpoint or start a new session\n",
    "\n",
    "        '''\n",
    "        if run_load_from_file:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(config.ckpt_dir))\n",
    "            training_epoch_loss, _ = pickle.load(open(config.ckpt_dir + '/pickle.pkl', 'rb'))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            training_epoch_loss = []\n",
    "\n",
    "        # Recording loss per epoch\n",
    "        process = Process()\n",
    "        for epoch in range(max_epoch):\n",
    "            process.start_epoch(epoch, max_epoch)\n",
    "\n",
    "            '''\n",
    "            Learning rate generator\n",
    "\n",
    "            '''\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "            # Recording loss per iteration\n",
    "            sum_loss_reconstruction = 0\n",
    "            sum_loss_discrminator_z = 0\n",
    "            sum_loss_discrminator_img = 0\n",
    "            sum_loss_generator_z = 0\n",
    "            sum_loss_generator_img = 0\n",
    "            process_iteration = Process()\n",
    "            for i in range(num_trains_per_epoch):\n",
    "                process_iteration.start_epoch(i, num_trains_per_epoch)\n",
    "                # Inputs\n",
    "                '''\n",
    "                _l -> labeled\n",
    "                _u -> unlabeled\n",
    "\n",
    "                '''\n",
    "                images_u = dataset.sample_unlabeled_data(images, batch_size_u)\n",
    "                if config.distribution_sampler == 'swiss_roll':\n",
    "                    z_true_u = sampler.swiss_roll(batch_size_u,\n",
    "                                                  config.ndim_z,\n",
    "                                                  config.num_types_of_label\n",
    "                                                  )\n",
    "                elif config.distribution_sampler == 'gaussian_mixture':\n",
    "                    z_true_u = sampler.gaussian_mixture(batch_size_u,\n",
    "                                                        config.ndim_z,\n",
    "                                                        config.num_types_of_label\n",
    "                                                        )\n",
    "                elif config.distribution_sampler == 'uniform_desk':\n",
    "                    z_true_u = sampler.uniform_desk(batch_size_u,\n",
    "                                                    config.ndim_z,\n",
    "                                                    radius=2\n",
    "                                                    )\n",
    "                elif config.distribution_sampler == 'gaussian':\n",
    "                    z_true_u = sampler.gaussian(batch_size_u,\n",
    "                                                config.ndim_z,\n",
    "                                                var=1\n",
    "                                                )\n",
    "                elif config.distribution_sampler == 'uniform':\n",
    "                    z_true_u = sampler.uniform(batch_size_u,\n",
    "                                               config.ndim_z,\n",
    "                                               minv=-1,\n",
    "                                               maxv=1\n",
    "                                               )\n",
    "\n",
    "                # reconstruction_phase\n",
    "                _, loss_reconstruction = sess.run([h.opt_r, h.loss_r], feed_dict={\n",
    "                    h.x: images_u,\n",
    "                    h.lr: learning_rate\n",
    "                })\n",
    "\n",
    "                # adversarial phase for discriminator_z\n",
    "                images_u_s = dataset.sample_unlabeled_data(images, batch_size_u)\n",
    "                _, loss_discriminator_z = sess.run([h.opt_dz, h.loss_dz], feed_dict={\n",
    "                    h.x: images_u,\n",
    "                    h.z: z_true_u,\n",
    "                    h.lr: learning_rate\n",
    "                })\n",
    "\n",
    "                _, loss_discriminator_img = sess.run([h.opt_dimg, h.loss_dimg], feed_dict={\n",
    "                    h.x: images_u,\n",
    "                    h.x_s: images_u_s,\n",
    "                    h.lr: learning_rate\n",
    "                })\n",
    "\n",
    "                # adversarial phase for generator\n",
    "                _, loss_generator_z = sess.run([h.opt_e, h.loss_e], feed_dict={\n",
    "                    h.x: images_u,\n",
    "                    h.lr: learning_rate\n",
    "                })\n",
    "\n",
    "                _, loss_generator_img = sess.run([h.opt_d, h.loss_d], feed_dict={\n",
    "                    h.x: images_u,\n",
    "                    h.lr: learning_rate\n",
    "                })\n",
    "\n",
    "                sum_loss_reconstruction += loss_reconstruction\n",
    "                sum_loss_discrminator_z += loss_discriminator_z\n",
    "                sum_loss_discrminator_img += loss_discriminator_img\n",
    "                sum_loss_generator_z += loss_generator_z\n",
    "                sum_loss_generator_img += loss_generator_img\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    process_iteration.show_table_2d(i, num_trains_per_epoch, {\n",
    "                        'reconstruction': sum_loss_reconstruction / (i + 1),\n",
    "                        'discriminator_z': sum_loss_discrminator_z / (i + 1),\n",
    "                        'discriminator_img': sum_loss_discrminator_img / (i + 1),\n",
    "                        'generator_z': sum_loss_generator_z / (i + 1),\n",
    "                        'generator_img': sum_loss_generator_img / (i + 1),\n",
    "                    })\n",
    "\n",
    "            average_loss_per_epoch = [\n",
    "                sum_loss_reconstruction / num_trains_per_epoch,\n",
    "                sum_loss_discrminator_z / num_trains_per_epoch,\n",
    "                sum_loss_discrminator_img / num_trains_per_epoch,\n",
    "                sum_loss_generator_z / num_trains_per_epoch,\n",
    "                sum_loss_generator_img / num_trains_per_epoch,\n",
    "                (sum_loss_discrminator_z+sum_loss_discrminator_img)/num_trains_per_epoch,\n",
    "                (sum_loss_generator_z + sum_loss_generator_img) / num_trains_per_epoch\n",
    "            ]\n",
    "            training_epoch_loss.append(average_loss_per_epoch)\n",
    "            training_loss_name = [\n",
    "                'reconstruction',\n",
    "                'discriminator_z',\n",
    "                'discriminator_img',\n",
    "                'generator_z',\n",
    "                'generator_img',\n",
    "                'discriminator',\n",
    "                'generator'\n",
    "            ]\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                process.show_bar(epoch, max_epoch, {\n",
    "                    'loss_r': average_loss_per_epoch[0],\n",
    "                    'loss_d': average_loss_per_epoch[5],\n",
    "                    'loss_g': average_loss_per_epoch[6]\n",
    "                })\n",
    "\n",
    "                plt.scatter_labeled_z(sess.run(h.z_r, feed_dict={h.x: images[:1000]}), [int(var) for var in labels[:1000]],\n",
    "                                      dir=config.log_dir,\n",
    "                                      filename='z_representation-{}'.format(epoch))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                saver.save(sess, os.path.join(config.ckpt_dir, 'model_ckptpoint'), global_step=epoch)\n",
    "                pickle.dump((training_epoch_loss, training_loss_name), open(config.ckpt_dir + '/pickle.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  0(500 in total)\t Time left:00:02:23\n",
      "---------------------------------------------------------------------------------------\n",
      "discriminator_img | discriminator_z |  generator_img  |   generator_z   | reconstruction \n",
      "---------------------------------------------------------------------------------------\n",
      "    1.37444     |     1.38615     |     0.69329     |     0.69299     |    362.95749   \n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch:   0/2010[...................................................................................................]--06:28:14-- - loss_d: 1.78471 -  - loss_g: 3.55127 -  - loss_r: 233.14348 - \n",
      "\n",
      "Iteration:  0(500 in total)\t Time left:00:00:13\n",
      "---------------------------------------------------------------------------------------\n",
      "discriminator_img | discriminator_z |  generator_img  |   generator_z   | reconstruction \n",
      "---------------------------------------------------------------------------------------\n",
      "    0.05329     |     1.00047     |     3.90422     |     1.15463     |    205.00098   \n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch:   1/2010[...................................................................................................]--06:13:35-- - loss_d: 1.32676 -  - loss_g: 6.28278 -  - loss_r: 147.23501 - \n",
      "\n",
      "Iteration:  0(500 in total)\t Time left:00:00:12\n",
      "---------------------------------------------------------------------------------------\n",
      "discriminator_img | discriminator_z |  generator_img  |   generator_z   | reconstruction \n",
      "---------------------------------------------------------------------------------------\n",
      "    0.07416     |     1.06106     |     5.62384     |     1.13956     |    122.26900   \n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch:   2/2010[...................................................................................................]--06:15:28-- - loss_d: 1.39333 -  - loss_g: 6.66363 -  - loss_r: 128.31045 - \n",
      "\n",
      "Iteration:  0(500 in total)\t Time left:00:00:12\n",
      "---------------------------------------------------------------------------------------\n",
      "discriminator_img | discriminator_z |  generator_img  |   generator_z   | reconstruction \n",
      "---------------------------------------------------------------------------------------\n",
      "    0.17416     |     1.21127     |     6.24784     |     0.93177     |    160.71956   \n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(run_load_from_file=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
