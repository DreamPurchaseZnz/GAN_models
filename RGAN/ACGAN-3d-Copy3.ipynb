{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as ly\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import _pickle as pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inputs:Global parameters \n",
    "batch_size = 32\n",
    "img_dim = 688\n",
    "z_dim = 110\n",
    "\n",
    "# Define salient variable\n",
    "cat_list = [] # Each entry in this list defines a categorical variable for specific size\n",
    "cont_num = 1 # the number of continous variables\n",
    "\n",
    "# Optimizer: whether use adam for parameters update , if flag is set false use tf.train.RMEPropOptimzer\n",
    "is_adam = True\n",
    "learning_rate_gen = 2e-4\n",
    "learning_rate_dis = 2e-4\n",
    "# the upper bound and lower bound in critic\n",
    "clamp_lower =-0.01\n",
    "clamp_upper = 0.01\n",
    "\n",
    "\n",
    "# Train : Prameters\n",
    "max_iter_step = 10000\n",
    "device = '/gpu:0'\n",
    "data_format = 'NCHW'\n",
    "channel = 1\n",
    "# Updata Diters times for critic in one iter(unless i<25 or i %500==0, i is the iterstep)\n",
    "Riters = 10\n",
    "Diters = 5\n",
    "Diters = int(((Riters+1)/Riters*Diters))\n",
    "\n",
    "# Loss balance parameter\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "# directory to store log ,include loss  and grad_norm of generator and critic\n",
    "rgan_dir = './Inproved_rgan/RGAN/Experient_3'\n",
    "log_dir = rgan_dir + '/log_rgan'\n",
    "ckpt_dir = rgan_dir + '/ckpt_rgan'\n",
    "spec_dir = rgan_dir + '/spec_rgan'\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "else:\n",
    "    shutil.rmtree(ckpt_dir)\n",
    "    os.makedirs(ckpt_dir)\n",
    "    \n",
    "if not os.path.exists(spec_dir):\n",
    "    os.makedirs(spec_dir)\n",
    "else:\n",
    "    shutil.rmtree(spec_dir)\n",
    "    os.makedirs(spec_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe1db5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from database load the data and label,clip the length network required\n",
    "spect = scio.loadmat('NMR_Index.mat')\n",
    "x,y = spect['NMR'],spect['Index'][:,3]\n",
    "if x.shape[1]>img_dim:\n",
    "    x = x[:,:img_dim]\n",
    "# plot raw data    \n",
    "plt.plot(x.transpose())\n",
    "plt.show()\n",
    "\n",
    "# pre_processing by minmaxmap\n",
    "xmin, xmax, ymin, ymax = x.min(),x.max(), y.min(),y.max()\n",
    "x,y = (x - xmin)/( xmax - xmin ),(y - ymin)/( ymax - ymin )\n",
    "\n",
    "# reshape the data to the format that look like image\n",
    "x= x.reshape(-1,1,img_dim,1)\n",
    "y= y.reshape(-1,cont_num)\n",
    "\n",
    "## split the dataset\n",
    "x_train,x_test,y_train,y_test =train_test_split(x,y,test_size=0.10,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator_conv(z):\n",
    "    train = ly.fully_connected(z, 1024, activation_fn = tf.nn.relu, normalizer_fn=None)\n",
    "    train = ly.fully_connected(train, 256*172, activation_fn = tf.nn.relu, normalizer_fn=None)\n",
    "    train = tf.reshape(train,(-1,256,172,1))\n",
    "    train = ly.conv2d_transpose(train, 128, kernel_size=[5,1],stride=[2,1],data_format=data_format,\n",
    "                                activation_fn = tf.nn.relu, normalizer_fn = ly.batch_norm, padding='SAME',\n",
    "                                normalizer_params={'fused':True,'data_format':data_format,},\n",
    "                                weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "    train = ly.conv2d_transpose(train, 64, kernel_size=[5,1],stride=[2,1],data_format=data_format,\n",
    "                                activation_fn = tf.nn.relu, normalizer_fn = ly.batch_norm, padding='SAME',\n",
    "                                normalizer_params={'fused':True,'data_format':data_format,},\n",
    "                                weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "    train = ly.conv2d_transpose(train, channel , kernel_size=[5,1],stride=[1,1],data_format=data_format,\n",
    "                                activation_fn = tf.nn.tanh, padding='SAME',\n",
    "                                weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x,leak=0.2,name='lrelu'):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5*(1+leak)\n",
    "        f2 = 0.5*(1-leak)\n",
    "        return f1*x + f2*abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def critic_conv( x ,cat_list =cat_list ,cont_num =cont_num,reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        size = 16\n",
    "        \n",
    "        img = ly.conv2d(x,num_outputs=size*4,kernel_size=[10,1],stride=[2,1],padding='SAME',\n",
    "                       activation_fn = lrelu,data_format=data_format,\n",
    "                       weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        \n",
    "        img = ly.conv2d(img,num_outputs= size*8 ,kernel_size=[10,1],stride=[2,1],padding='SAME',\n",
    "                        activation_fn = lrelu,data_format=data_format,normalizer_fn = ly.batch_norm,\n",
    "                        normalizer_params ={'fused':True,'data_format':data_format},\n",
    "                        weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        \n",
    "        img = ly.conv2d(img,num_outputs= size*16,kernel_size=[10,1],stride=[2,1],padding='SAME',\n",
    "                        activation_fn = lrelu,data_format=data_format,normalizer_fn = ly.batch_norm,\n",
    "                        normalizer_params ={'fused':True,'data_format':data_format},\n",
    "                        weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        \n",
    "        \n",
    "        # fully_connected\n",
    "        disc_img = tf.reshape(img,[batch_size,-1])\n",
    "        disc_img = ly.fully_connected(disc_img,1024,activation_fn=lrelu)\n",
    "        disc = ly.fully_connected(disc_img,1,activation_fn=None)\n",
    "        \n",
    "        # Here we difine the unique layers used for the q_network, the number of outputs \n",
    "        q_img = ly.conv2d(img,num_outputs= size*32,kernel_size=[10,1],stride=[2,1],padding='SAME',\n",
    "                        activation_fn = lrelu,data_format=data_format,normalizer_fn = ly.batch_norm,\n",
    "                        normalizer_params ={'fused':True,'data_format':data_format},\n",
    "                        weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        \n",
    "        q_img = tf.reshape(q_img,[batch_size,-1])\n",
    "        q_img = ly.fully_connected(q_img,1024,activation_fn=lrelu)\n",
    "        \n",
    "        q_cat_outs = []\n",
    "        if len(cat_list)>0:\n",
    "            for idx,var in enumerate(cat_list):\n",
    "                q_outA = ly.fully_connected(q_img,var, activation_fn=tf.nn.softmax)\n",
    "                q_cat_outs.append(q_outA)\n",
    "        \n",
    "        q_cont_outs = []\n",
    "        if cont_num>0:\n",
    "            q_cont_outs = ly.fully_connected(q_img,cont_num,activation_fn = tf.nn.tanh)\n",
    "    \n",
    "    return disc, q_cat_outs, q_cont_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph & Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_test=False):\n",
    "    ##########################  Inputs ######################################\n",
    "    # Real data \n",
    "    real_data = tf.placeholder(dtype = tf.float32,shape=[batch_size,channel,img_dim,1])\n",
    "    real_label = tf.placeholder(dtype = tf.float32,shape = [batch_size,cont_num])\n",
    "    # Noise ;Continous variable, and rand variable to provide some variance\n",
    "    z_cont = tf.placeholder(dtype = tf.float32,shape =[batch_size,cont_num])\n",
    "    z_rand = tf.placeholder(dtype =tf.float32,shape =[batch_size,z_dim])\n",
    "    # Categorical variable\n",
    "    if len(cat_list)>0:\n",
    "        z_cat = tf.placeholder(dtype = tf.int32, shape=[batch_size,len(cat_list)])\n",
    "        cat_var = tf.split(z_cat , len(cat_list),axis=1)\n",
    "\n",
    "        cat_z =[]\n",
    "        for idx ,depth in enumerate(cat_list):\n",
    "            cat_temp = tf.one_hot(tf.reshape(cat_var[idx],[-1]), depth)\n",
    "            cat_z.append(cat_temp)\n",
    "    else:\n",
    "        z_cat = tf.placeholder(dtype =tf.float32,shape =[batch_size,10])\n",
    "        cat_z = [z_cat]\n",
    "    # Conbine all of the variable\n",
    "    z = cat_z[:]\n",
    "    z.append(z_cont)\n",
    "    z.append(z_rand)\n",
    "    z = tf.concat( z,axis=1)\n",
    "    \n",
    "    #########################  End Inputs #####################################\n",
    "    \n",
    "    ##########################  Graph   ######################################\n",
    "    generator = generator_conv\n",
    "    discriminator = critic_conv\n",
    "    \n",
    "    with tf.variable_scope('generator'):\n",
    "        gen = generator(z)\n",
    "    \n",
    "    if is_test:\n",
    "        return gen,z_rand,z_cont,z_cat\n",
    "    \n",
    "    disc_real,cat_real,cont_real = discriminator(real_data)\n",
    "    disc_fake,cat_fake,cont_fake = discriminator(gen, reuse=True)\n",
    "\n",
    "    # Wasserstein distance\n",
    "    d_loss = tf.reduce_mean(disc_fake - disc_real)\n",
    "    g_loss = tf.reduce_mean( - disc_fake)\n",
    "    g_loss_summ = tf.summary.scalar('wasserstein_loss_g' , g_loss)\n",
    "    d_loss_summ = tf.summary.scalar('wasserstein_loss_d' , -d_loss)\n",
    "    \n",
    "    # Combine losses for each of categorical variables\n",
    "    if len(cat_list)>0:\n",
    "        cat_losses_f =[]\n",
    "        tiny = 1e-10\n",
    "        for idx,var in enumerate(cat_z):\n",
    "            cat_loss_f = -tf.reduce_sum(var *tf.log(cat_fake[idx]+tiny),reduction_indices=1)\n",
    "            cat_losses_f.append(cat_loss_f)\n",
    "            # cat_losses_r \n",
    "    else:\n",
    "        cat_losses_f=cat_losses_r = tf.constant(0.0)\n",
    "    # Combine losses for each of continous variables\n",
    "    # MSE\n",
    "    if cont_num>0:\n",
    "        cont_loss_f = tf.reduce_sum(0.5*tf.square(cont_fake-z_cont),reduction_indices=1)\n",
    "        cont_loss_r = tf.reduce_sum(0.5*tf.square(cont_real- real_label),reduction_indices=1)\n",
    "    else:\n",
    "        cont_loss_f = cont_loss_r = tf.constant(0.0)\n",
    "    \n",
    "    q_cont_loss = tf.reduce_mean(cont_loss_r) + alpha* tf.reduce_mean(cont_loss_f)\n",
    "    q_cat_loss = tf.constant(0.0)\n",
    "    q_loss = tf.add(q_cont_loss,q_cat_loss)\n",
    "    \n",
    "    # Loss of Discriminator and Generator, Discriminator train on True and False data\n",
    "    loss_gen = g_loss\n",
    "    loss_dis = d_loss\n",
    "    loss_qnt = q_loss\n",
    "    \n",
    "    loss_gen_summ = tf.summary.scalar('generator_loss',loss_gen)\n",
    "    loss_dis_summ = tf.summary.scalar('discriminator_loss',loss_dis)\n",
    "    ########################### End Graph   ######################################\n",
    "    \n",
    "    \n",
    "    ##################### Optimization ########################################\n",
    "    # Variable Collection\n",
    "    theta_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='generator')\n",
    "    theta_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='discriminator')\n",
    "    theta_c = theta_d[10:]\n",
    "\n",
    "    # Optimizers\n",
    "    counter_g = tf.Variable(trainable=False , initial_value=0,dtype=tf.int32)\n",
    "    opt_g = optimize(loss = loss_gen,variables=theta_g, learning_rate = learning_rate_gen,\n",
    "                    optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                    global_step=counter_g,summaries=['gradient_norm','gradients'])\n",
    "    \n",
    "    counter_d = tf.Variable(trainable=False , initial_value=0,dtype=tf.int32)\n",
    "    opt_d = optimize(loss =loss_dis,variables=theta_d ,learning_rate = learning_rate_dis,\n",
    "                    optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                    global_step=counter_d,summaries=['gradient_norm','gradients'])\n",
    "    \n",
    "    counter_q = tf.Variable(trainable=False , initial_value=0,dtype=tf.int32)\n",
    "    opt_q = optimize(loss =loss_qnt ,variables=theta_c+theta_g ,learning_rate = learning_rate_dis,\n",
    "                    optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                    global_step=counter_d,summaries=['gradient_norm','gradients'])\n",
    "    # Clip weights\n",
    "    clipped_var_d = [tf.assign(var,tf.clip_by_value(var,clamp_lower,clamp_upper)) for var in theta_d]\n",
    "    with tf.control_dependencies([opt_d]):\n",
    "        opt_d = tf.tuple(clipped_var_d)\n",
    "    ################################# End Optimization ################################################\n",
    "    return opt_d, opt_g ,opt_q,real_data,real_label,z_rand,z_cont,z_cat,loss_dis,loss_gen,loss_qnt,gen,q_cont_loss,q_cat_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss,learning_rate, optimizer,variables,global_step,summaries):\n",
    "    optim = optimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Caculate gradients\n",
    "    gradients = optim.compute_gradients(loss,var_list = variables)\n",
    "    \n",
    "    # Add summary\n",
    "    if summaries is None:\n",
    "        summaries = ['loss','learning_rate']\n",
    "    if 'gradient_norm'  in summaries:\n",
    "        tf.summary.scalar('global_norm/gradient_norm',tf.global_norm(list(zip(*gradients))[0]))\n",
    "        \n",
    "    if 'loss' in summaries:\n",
    "        tf.summary.scalar('loss',loss)\n",
    "    \n",
    "    for gradient,variable in gradients:\n",
    "        if isinstance(gradient,tf.IndexedSlices):\n",
    "            grad_values = gradient.values\n",
    "        else:\n",
    "            grad_values = gradient\n",
    "        \n",
    "        if grad_values is not None:\n",
    "            var_name = variable.name.replace(':','_')\n",
    "            if 'gradients' in summaries:\n",
    "                tf.summary.histogram('gradients/%s'%var_name, grad_values)\n",
    "            \n",
    "            if 'gradient_norm' in summaries:\n",
    "                tf.summary.scalar('gradient_norm/%s'%var_name,tf.global_norm([grad_values]))\n",
    "    return optim.apply_gradients(gradients,global_step=global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(specs ,size,i =100,image_path = rgan_dir):\n",
    "    _,ax = plt.subplots(size[0],size[1],sharex = True,sharey = True)\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            ax[i][j].plot(np.arange(specs.shape[1]),specs[i*10+j] ,'-*')\n",
    "    \n",
    "    plt.savefig(image_path+'/'+str(i).zfill(4)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_spectrum(gen, i,spec_size = 10,is_show=False):\n",
    "    if gen.shape[0]<spec_size:\n",
    "        spec_size = gen.shape[0]\n",
    "\n",
    "    gen = gen[:spec_size]\n",
    "    # control the spect limitation and intv determine the the distance of spectrums\n",
    "    intv = 0\n",
    "    scale = 3.9\n",
    "    # extend along y axis to visulize the difference between spetrum\n",
    "    if is_show:\n",
    "        extd = np.matlib.repmat(np.arange(0,spec_size),img_dim,1) \n",
    "        extd = extd * scale*intv\n",
    "    else:\n",
    "        extd = np.zeros((img_dim, spec_size))\n",
    "    # convert data to ideal format    \n",
    "    spec = gen.reshape(spec_size,img_dim)*(xmax-xmin)+ xmin\n",
    "    spec = spec.transpose() + extd\n",
    "    \n",
    "    # plot the spectrum with ylimit legend and other thing\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(np.arange(spec.shape[0]),spec ,'-*')    \n",
    "    plt.legend([np.array_str(var) for var in np.arange(spec_size)])\n",
    "    if intv ==0:\n",
    "        plt.ylim([-0.5,scale])\n",
    "    else:\n",
    "        plt.ylim([-0.5,scale*spec_size*intv])\n",
    "    \n",
    "    \n",
    "    if is_show:\n",
    "        return plt.show()\n",
    "    # savefig\n",
    "    plt.savefig(spec_dir+'/'+str(i).zfill(4)+'.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condictional WGAN Subphase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_rgan(max_iters = 1000):\n",
    "    # build graph\n",
    "    with tf.device(device):\n",
    "        opt_d, opt_g ,opt_q,real_data,real_label,z_rand,z_cont,z_cat,\\\n",
    "        loss_dis,loss_gen,loss_qnt,gen,q_cont_loss,q_cat_loss = build_graph()\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    def next_feed_dict(iter, shuffled = False,is_test =False):\n",
    "        \n",
    "        # feed image and label,train set if is_test=False\n",
    "        # otherwise test set\n",
    "        def generate_index(input,iter,shuffled =False):\n",
    "            input_number = input.shape[0]\n",
    "            if shuffled:\n",
    "                idx = np.random.randint(input_number,size=(batch_size,))\n",
    "            else:\n",
    "                bat_num = int(input_number/batch_size)\n",
    "                par = int(iter % bat_num)\n",
    "                idx = np.arange(input_number)[par * batch_size:(par+1)*batch_size]\n",
    "            return idx\n",
    "        \n",
    "        if is_test:\n",
    "            idx = generate_index(x_train,iter)\n",
    "            feed_img,feed_label = x_train[idx],y_train[idx]\n",
    "        else:\n",
    "            idx = generate_index(x_test,iter)\n",
    "            feed_img,feed_label = x_test[idx],y_test[idx]\n",
    "        \n",
    "        # False data which sigma = 0.3 mean most of point located in the [-1,1].\n",
    "        batch_z_rand = np.random.normal(0,0.3,[batch_size,z_dim]).astype(np.float32)\n",
    "        if len(cat_list)>0:\n",
    "            batch_z_cat  = np.random.randint(0,10,[batch_size,len(cat_list)])\n",
    "        else:\n",
    "            batch_z_cat = np.random.normal(0,0.3,[batch_size,10]).astype(np.float32)\n",
    "        \n",
    "        batch_z_cont = np.random.uniform(-1,1,[batch_size,cont_num])     \n",
    "        \n",
    "        feed_dict = {real_data:feed_img, \n",
    "                     z_rand : batch_z_rand,\n",
    "                     z_cat  : batch_z_cat,\n",
    "                     z_cont : batch_z_cont,\n",
    "                     real_label:feed_label }\n",
    "        return feed_dict\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True,log_device_placement=True)\n",
    "    config.gpu_options.allow_growth =True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=0.9\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # use two dict-like container to record of history data\n",
    "        # for analysising the result\n",
    "        history = defaultdict(list)\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            \n",
    "            ## Training Phase\n",
    "            feed_dict = next_feed_dict(i)\n",
    "            \n",
    "            _,dLoss = sess.run([opt_d,loss_dis],feed_dict = feed_dict)\n",
    "            _,gLoss,spec = sess.run([opt_g,loss_gen,gen],feed_dict = feed_dict)\n",
    "            _,qLoss,qCont,qCat = sess.run([opt_q, loss_qnt,q_cont_loss,q_cat_loss],feed_dict = feed_dict)\n",
    "            \n",
    "            history['train_loss'].append([dLoss,gLoss,qLoss,qCont,qCat])\n",
    "            \n",
    "            ## Testing Phase\n",
    "            feed_dict = next_feed_dict(i,is_test=True)\n",
    "        \n",
    "            dLoss_ = sess.run(loss_dis,feed_dict = feed_dict)\n",
    "            gLoss_ = sess.run(loss_gen,feed_dict = feed_dict)\n",
    "            qLoss_ ,qCont_ ,qCat_ = sess.run([loss_qnt,q_cont_loss,q_cat_loss],feed_dict = feed_dict)\n",
    "            \n",
    "            history['test_loss'].append([dLoss_ ,gLoss_ ,qLoss_ ,qCont_ ,qCat_])\n",
    "            if i % 100 == 0:\n",
    "                print('\\nIteration: {:0>5d}({:0.2%}) of {}\\n'.format(i,i/max_iters,max_iters))\n",
    "                \n",
    "                print('{0:<8s} | {1:<15s} | {2:<15s} | {3:<15s} | {4:<15s} | {5:<15s}'\\\n",
    "                      .format('Loss','generator','discriminator','Q network','Q_continous','Q_categorical'))\n",
    "                print('-' * 98)\n",
    "                row_fmt = '{0:<8s} | {1:<15.3f} | {2:<15.3f} | {3:<15.3f} | {4:<15.3f} | {5:<15.3f}'\n",
    "                print(row_fmt.format('train',*history['train_loss'][-1]))\n",
    "                print(row_fmt.format('test' ,*history['test_loss'][-1]))\n",
    "                save_spectrum(spec, i)\n",
    "            \n",
    "\n",
    "            if i %1000 ==0:\n",
    "                saver.save(sess, os.path.join(ckpt_dir,'model_checkpoint'),global_step=i)\n",
    "            \n",
    "        pickle.dump(history,open(rgan_dir+'/rgan_pickle_alpha_{}.pkl'.format(alpha),'wb'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 00000(0.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | 0.060           | 0.613           | 0.307           | 0.307           | 0.000          \n",
      "test     | -0.246          | -0.386          | 0.159           | 0.159           | 0.000          \n",
      "\n",
      "Iteration: 00100(0.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -164.329        | 69.955          | 0.241           | 0.241           | 0.000          \n",
      "test     | -187.404        | 58.784          | 0.179           | 0.179           | 0.000          \n",
      "\n",
      "Iteration: 00200(1.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -875.121        | 457.656         | 0.154           | 0.154           | 0.000          \n",
      "test     | -918.054        | 449.756         | 0.135           | 0.135           | 0.000          \n",
      "\n",
      "Iteration: 00300(2.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -488.589        | 28.159          | 0.160           | 0.160           | 0.000          \n",
      "test     | -472.902        | -31.165         | 0.142           | 0.142           | 0.000          \n",
      "\n",
      "Iteration: 00400(3.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -529.786        | 52.871          | 0.170           | 0.170           | 0.000          \n",
      "test     | -560.198        | 25.647          | 0.130           | 0.130           | 0.000          \n",
      "\n",
      "Iteration: 00500(4.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -908.261        | 354.486         | 0.168           | 0.168           | 0.000          \n",
      "test     | -911.921        | 332.472         | 0.181           | 0.181           | 0.000          \n",
      "\n",
      "Iteration: 00600(5.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -920.760        | 429.032         | 0.164           | 0.164           | 0.000          \n",
      "test     | -965.090        | 416.878         | 0.105           | 0.105           | 0.000          \n",
      "\n",
      "Iteration: 00700(5.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1072.272       | 598.083         | 0.154           | 0.154           | 0.000          \n",
      "test     | -1132.117       | 596.152         | 0.141           | 0.141           | 0.000          \n",
      "\n",
      "Iteration: 00800(6.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -695.244        | 166.882         | 0.244           | 0.244           | 0.000          \n",
      "test     | -699.720        | 142.916         | 0.171           | 0.171           | 0.000          \n",
      "\n",
      "Iteration: 00900(7.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -777.709        | 271.823         | 0.165           | 0.165           | 0.000          \n",
      "test     | -803.817        | 262.330         | 0.170           | 0.170           | 0.000          \n",
      "\n",
      "Iteration: 01000(8.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -914.119        | 397.876         | 0.159           | 0.159           | 0.000          \n",
      "test     | -968.095        | 396.373         | 0.150           | 0.150           | 0.000          \n",
      "\n",
      "Iteration: 01100(9.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -996.418        | 428.602         | 0.158           | 0.158           | 0.000          \n",
      "test     | -1030.373       | 430.261         | 0.134           | 0.134           | 0.000          \n",
      "\n",
      "Iteration: 01200(10.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -866.093        | 263.261         | 0.151           | 0.151           | 0.000          \n",
      "test     | -893.751        | 262.924         | 0.081           | 0.081           | 0.000          \n",
      "\n",
      "Iteration: 01300(10.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -907.263        | 268.268         | 0.151           | 0.151           | 0.000          \n",
      "test     | -918.852        | 266.701         | 0.118           | 0.118           | 0.000          \n",
      "\n",
      "Iteration: 01400(11.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -910.794        | 288.577         | 0.150           | 0.150           | 0.000          \n",
      "test     | -950.687        | 288.294         | 0.113           | 0.113           | 0.000          \n",
      "\n",
      "Iteration: 01500(12.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -943.806        | 285.770         | 0.150           | 0.150           | 0.000          \n",
      "test     | -956.825        | 285.708         | 0.135           | 0.135           | 0.000          \n",
      "\n",
      "Iteration: 01600(13.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -947.202        | 320.197         | 0.152           | 0.152           | 0.000          \n",
      "test     | -988.625        | 318.374         | 0.131           | 0.131           | 0.000          \n",
      "\n",
      "Iteration: 01700(14.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -931.607        | 290.659         | 0.149           | 0.149           | 0.000          \n",
      "test     | -972.349        | 290.765         | 0.150           | 0.150           | 0.000          \n",
      "\n",
      "Iteration: 01800(15.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -931.172        | 289.535         | 0.150           | 0.150           | 0.000          \n",
      "test     | -972.690        | 289.913         | 0.171           | 0.171           | 0.000          \n",
      "\n",
      "Iteration: 01900(15.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -976.567        | 298.266         | 0.153           | 0.153           | 0.000          \n",
      "test     | -986.789        | 297.710         | 0.095           | 0.095           | 0.000          \n",
      "\n",
      "Iteration: 02000(16.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -932.781        | 287.812         | 0.153           | 0.153           | 0.000          \n",
      "test     | -969.266        | 282.038         | 0.139           | 0.139           | 0.000          \n",
      "\n",
      "Iteration: 02100(17.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -958.846        | 301.006         | 0.151           | 0.151           | 0.000          \n",
      "test     | -998.504        | 300.959         | 0.118           | 0.118           | 0.000          \n",
      "\n",
      "Iteration: 02200(18.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1007.074       | 322.220         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1020.117       | 322.164         | 0.154           | 0.154           | 0.000          \n",
      "\n",
      "Iteration: 02300(19.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -982.092        | 320.406         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1024.285       | 320.054         | 0.149           | 0.149           | 0.000          \n",
      "\n",
      "Iteration: 02400(20.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1005.052       | 309.773         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1017.620       | 309.644         | 0.114           | 0.114           | 0.000          \n",
      "\n",
      "Iteration: 02500(20.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -964.564        | 303.676         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1006.624       | 302.537         | 0.083           | 0.083           | 0.000          \n",
      "\n",
      "Iteration: 02600(21.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1011.587       | 314.670         | 0.149           | 0.149           | 0.000          \n",
      "test     | -1024.842       | 314.537         | 0.112           | 0.112           | 0.000          \n",
      "\n",
      "Iteration: 02700(22.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1005.020       | 322.671         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1033.722       | 322.591         | 0.114           | 0.114           | 0.000          \n",
      "\n",
      "Iteration: 02800(23.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -992.766        | 323.922         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1035.742       | 323.885         | 0.132           | 0.132           | 0.000          \n",
      "\n",
      "Iteration: 02900(24.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -991.782        | 320.472         | 0.149           | 0.149           | 0.000          \n",
      "test     | -1033.836       | 320.461         | 0.131           | 0.131           | 0.000          \n",
      "\n",
      "Iteration: 03000(25.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -994.975        | 325.192         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1037.774       | 325.174         | 0.124           | 0.124           | 0.000          \n",
      "\n",
      "Iteration: 03100(25.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -994.005        | 321.688         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1036.247       | 321.695         | 0.171           | 0.171           | 0.000          \n",
      "\n",
      "Iteration: 03200(26.67%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -994.775        | 322.409         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1033.396       | 322.151         | 0.096           | 0.096           | 0.000          \n",
      "\n",
      "Iteration: 03300(27.50%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1009.825       | 322.801         | 0.151           | 0.151           | 0.000          \n",
      "test     | -1037.646       | 322.575         | 0.138           | 0.138           | 0.000          \n",
      "\n",
      "Iteration: 03400(28.33%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1025.706       | 325.322         | 0.149           | 0.149           | 0.000          \n",
      "test     | -1037.080       | 325.208         | 0.118           | 0.118           | 0.000          \n",
      "\n",
      "Iteration: 03500(29.17%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -999.808        | 326.435         | 0.150           | 0.150           | 0.000          \n",
      "test     | -1040.037       | 326.150         | 0.151           | 0.151           | 0.000          \n",
      "\n",
      "Iteration: 03600(30.00%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1026.272       | 326.671         | 0.152           | 0.152           | 0.000          \n",
      "test     | -1039.975       | 326.559         | 0.151           | 0.151           | 0.000          \n",
      "\n",
      "Iteration: 03700(30.83%) of 12000\n",
      "\n",
      "Loss     | generator       | discriminator   | Q network       | Q_continous     | Q_categorical  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "train    | -1015.459       | 329.939         | 0.149           | 0.149           | 0.000          \n",
      "test     | -1043.914       | 329.890         | 0.115           | 0.115           | 0.000          \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-70b83bfac6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_rgan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-d5db3c419680>\u001b[0m in \u001b[0;36mtrain_rgan\u001b[0;34m(max_iters)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_dis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqCont\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqCat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_qnt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq_cont_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq_cat_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqCont\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqCat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "max_iters = 12000\n",
    "train_rgan(max_iters =max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Inproved_rgan/RGAN/Experient_3/rgan_pickle_alpha_0.1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-36f10564c7c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgan_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/rgan_pickle_alpha_{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Inproved_rgan/RGAN/Experient_3/rgan_pickle_alpha_0.1.pkl'"
     ]
    }
   ],
   "source": [
    "hist = pickle.load(open(rgan_dir+'/rgan_pickle_alpha_{}.pkl'.format(alpha),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = ['generator','discriminator','Q network','Q_continous','Q_categorical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to realize the two demensional dict-like tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in hist.keys():\n",
    "    hist[p] = pd.DataFrame(hist[p],columns=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in hist.keys():\n",
    "    plt.plot(hist[p]['discriminator'],label = '{}'.format(p))\n",
    "plt.legend(['train loss','test loss'])\n",
    "plt.title('Discriminator loss per iteration')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'Loss $L_Q$ ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in hist.keys():\n",
    "    plt.plot(hist[p]['generator'],label = '{}'.format(p))\n",
    "plt.legend(['train loss','test loss'])\n",
    "plt.title('Generator loss per iteration')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'Loss $L_Q$ ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in hist.keys():\n",
    "    plt.plot(hist[p]['Q network'],label = '{}'.format(p))\n",
    "plt.legend(['train loss','test loss'])\n",
    "plt.title('Q network loss per iteration')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'Loss $L_Q$ ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(ckpt_dir=ckpt_dir):\n",
    "    \n",
    "    with tf.Session(config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n",
    "        # build graph\n",
    "        with tf.device(device):\n",
    "            gen,z_rand,z_cont,z_cat = build_graph(is_test=True)\n",
    "        \n",
    "        # use saver to reload the parametes\n",
    "        saver = tf.train.Saver()\n",
    "        #saver.restore(sess, './Inproved_rgan/cgan_2d/Experient_1/ckpt_rgan\\\\model_checkpoint-17000')\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(ckpt_dir))\n",
    "        \n",
    "        ###########################################  Experiment : Generator #################\n",
    "        # Gererate random labels as conditions , [0,1] gausson distribution\n",
    "        z_rand_sample = np.random.normal(0,0.3,[batch_size,z_dim])\n",
    "        # To construct grid sample \n",
    "        #z_cat_sample = np.reshape(np.array([int(e/batch_size*10) for e in range(batch_size)]),[batch_size,len(cat_list)])                \n",
    "        z_cat_sample = np.random.normal(0,0.3,[batch_size,10])\n",
    "        #z_cont_sample = np.zeros((batch_size,cont_num))\n",
    "        z_cont_order = np.linspace(1,1,batch_size)\n",
    "        if cont_num>0:\n",
    "            z_cont_temp = np.zeros((batch_size,cont_num-1))\n",
    "            z_cont_sample = np.insert(z_cont_temp,0,z_cont_order,axis=1)\n",
    "        else:\n",
    "            z_cont_sample = z_cont_oder\n",
    "\n",
    "        samples = sess.run(gen,{z_rand:z_rand_sample,z_cat:z_cat_sample,z_cont:z_cont_sample})\n",
    "\n",
    "        save_spectrum(samples, 0,is_show=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
