{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.layers as ly\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator_conv(z):\n",
    "    train = ly.fully_connected(z,4 * 4 * 512, activation_fn=tf.nn.relu, normalizer_fn=None)\n",
    "    train = tf.reshape(train, (-1, 512, 4, 4))\n",
    "    train = ly.conv2d_transpose(train, 256, 3, stride=2, data_format=data_format,\n",
    "                                activation_fn=tf.nn.relu, normalizer_fn=ly.batch_norm, padding='SAME',\n",
    "                                normalizer_params={'fused': True, 'data_format': data_format},\n",
    "                                weights_initializer=tf.random_normal_initializer(0, 0.04))\n",
    "    train = ly.conv2d_transpose(train, 128, 3, stride=2, data_format=data_format,\n",
    "                                activation_fn=tf.nn.relu, normalizer_fn=ly.batch_norm, padding='SAME',\n",
    "                                normalizer_params={'fused': True, 'data_format': data_format},\n",
    "                                weights_initializer=tf.random_normal_initializer(0, 0.04))\n",
    "    train = ly.conv2d_transpose(train, 64, 3, stride=2, data_format=data_format,\n",
    "                                activation_fn=tf.nn.relu, normalizer_fn=ly.batch_norm, padding='SAME',\n",
    "                                normalizer_params={'fused': True, 'data_format': data_format},\n",
    "                                weights_initializer=tf.random_normal_initializer(0, 0.04))\n",
    "    train = ly.conv2d_transpose(train, channel, 3, stride=1, data_format=data_format,\n",
    "                                activation_fn=tf.nn.tanh, padding='SAME',\n",
    "                                weights_initializer=tf.random_normal_initializer(0, 0.04))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic_conv(x, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        size = 64\n",
    "\n",
    "        img = ly.conv2d(x, num_outputs=size, kernel_size=3, data_format=data_format,\n",
    "                        stride=2, activation_fn=lrelu)\n",
    "        img = ly.conv2d(img, num_outputs=size * 2, kernel_size=3, data_format=data_format,\n",
    "                        stride=2, activation_fn=lrelu, normalizer_fn=ly.batch_norm,\n",
    "                        normalizer_params={'fused': True, 'data_format': data_format})\n",
    "        img = ly.conv2d(img, num_outputs=size * 4, kernel_size=3, data_format=data_format,\n",
    "                        stride=2, activation_fn=lrelu, normalizer_fn=ly.batch_norm,\n",
    "                        normalizer_params={'fused': True, 'data_format': data_format})\n",
    "\n",
    "        img = ly.conv2d(img, num_outputs=size * 8, kernel_size=3, data_format=data_format,\n",
    "                        stride=2, activation_fn=lrelu, normalizer_fn=ly.batch_norm,\n",
    "                        normalizer_params={'fused': True, 'data_format': data_format})\n",
    "        img = tf.reshape(img, [batch_size, -1])\n",
    "\n",
    "        disc = ly.fully_connected(img, 1, activation_fn=None)\n",
    "    return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_conv(x, reuse=False):\n",
    "    with tf.variable_scope('classifier') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        size = 32\n",
    "\n",
    "        img = ly.conv2d(x, num_outputs=size, kernel_size=5, data_format=data_format,\n",
    "                        stride=1, activation_fn=tf.nn.relu)\n",
    "        img = ly.max_pool2d(img, 2, stride=2, padding='SAME', data_format=data_format)\n",
    "        img = ly.conv2d(img, num_outputs=size * 2, kernel_size=5, data_format=data_format,\n",
    "                        stride=1, activation_fn=tf.nn.relu, normalizer_fn=ly.batch_norm,\n",
    "                        normalizer_params={'fused': True, 'data_format': data_format})\n",
    "        img = ly.max_pool2d(img, 2, stride=2, padding='SAME', data_format=data_format)\n",
    "\n",
    "        img = tf.reshape(img, [batch_size, -1])\n",
    "\n",
    "        # fully connect\n",
    "        img = ly.fully_connected(img, 1024, activation_fn=tf.nn.relu)\n",
    "\n",
    "        cat = ly.fully_connected(img, y_dim, activation_fn=None)\n",
    "    return cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_test=False):\n",
    "\n",
    "    ############################ Inputs ##################################\n",
    "    # real data\n",
    "    real_data = tf.placeholder(\n",
    "        dtype=tf.float32, shape=(batch_size, channel, img_dim, img_dim))\n",
    "    real_label = tf.placeholder(dtype=tf.float32, shape=(batch_size, y_dim))\n",
    "\n",
    "    # fake data\n",
    "    z_cat = tf.placeholder(dtype=tf.float32, shape=(batch_size, y_dim))\n",
    "    z_rand = tf.placeholder(tf.float32, shape=(batch_size, z_dim))\n",
    "    z = tf.concat([z_cat, z_rand], axis=1)\n",
    "    ########################### End Inputs ###############################\n",
    "\n",
    "    ############################# Graph #################################\n",
    "    generator = generator_conv\n",
    "    discriminator = critic_conv\n",
    "    classifier = classifier_conv\n",
    "\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator'):\n",
    "        gen = generator(z)\n",
    "    if is_test:\n",
    "        return gen, z_cat, z_rand\n",
    "\n",
    "    # image summary\n",
    "    img_sum = tf.summary.image(\"img\", tf.transpose(gen, (0, 2, 3, 1)), max_outputs=10)\n",
    "\n",
    "    # Discriminator\n",
    "    disc_real = discriminator(real_data)\n",
    "    disc_fake = discriminator(gen, reuse=True)\n",
    "\n",
    "    # Classifier\n",
    "    cat_real = classifier(real_data)\n",
    "    cat_fake = classifier(gen, reuse=True)\n",
    "\n",
    "    # Loss\n",
    "    # Wasserstein\n",
    "    d_loss = tf.reduce_mean(disc_fake - disc_real)\n",
    "    g_loss = tf.reduce_mean(-disc_fake)\n",
    "    g_loss_sum = tf.summary.scalar(\"wasserstein_loss_g\", g_loss)\n",
    "    c_loss_sum = tf.summary.scalar(\"wasserstein_loss_d\", d_loss)\n",
    "\n",
    "    # Categorical Loss\n",
    "    loss_c_f = tf.nn.softmax_cross_entropy_with_logits(logits=cat_fake, labels=z_cat)\n",
    "    loss_c_f_sum = tf.summary.scalar(\"categorical_loss_c_fake\", tf.reduce_mean(loss_c_f))\n",
    "    loss_c_r = tf.nn.softmax_cross_entropy_with_logits(logits=cat_real, labels=real_label)\n",
    "    loss_c_r_sum = tf.summary.scalar(\"categorical_loss_c_real\", tf.reduce_mean(loss_c_r))\n",
    "    loss_c = (loss_c_r + loss_c_f) / 2\n",
    "    loss_c_sum = tf.summary.scalar(\"categorical_loss_c\", tf.reduce_mean(loss_c))\n",
    "    ############################# End Graph ##############################\n",
    "\n",
    "    ############################# Optimization #################################\n",
    "    # Variable Collections\n",
    "    theta_g = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    theta_d = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "    theta_c = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='classifier')\n",
    "\n",
    "    # Optimizers\n",
    "    counter_g = tf.Variable(trainable=False, initial_value=0, dtype=tf.int32)\n",
    "    opt_g = optimize(loss=g_loss, learning_rate=learning_rate_ger,\n",
    "                     optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                     variables=theta_g, global_step=counter_g,\n",
    "                     summaries='gradient_norm')\n",
    "    counter_d = tf.Variable(trainable=False, initial_value=0, dtype=tf.int32)\n",
    "    opt_d = optimize(loss=d_loss, learning_rate=learning_rate_dis,\n",
    "                     optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                     variables=theta_d, global_step=counter_d,\n",
    "                     summaries='gradient_norm')\n",
    "    counter_c_f = tf.Variable(trainable=False, initial_value=0, dtype=tf.int32)\n",
    "    opt_c_f = optimize(loss=loss_c_f, learning_rate=learning_rate_ger,\n",
    "                       optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                       variables=theta_g, global_step=counter_c_f,\n",
    "                       summaries='gradient_norm')\n",
    "    counter_c_r = tf.Variable(trainable=False, initial_value=0, dtype=tf.int32)\n",
    "    opt_c_r = optimize(loss=loss_c, learning_rate=learning_rate_cat,\n",
    "                       optimizer=tf.train.AdamOptimizer if is_adam is True else tf.train.RMSPropOptimizer,\n",
    "                       variables=theta_c, global_step=counter_c_r,\n",
    "                       summaries='gradient_norm')\n",
    "\n",
    "    # Clip weights\n",
    "    clipped_var_d = [tf.assign(var, tf.clip_by_value(var, clamp_lower, clamp_upper)) for var in theta_d]\n",
    "    # merge the clip operations on discriminator variables\n",
    "    with tf.control_dependencies([opt_d]):\n",
    "        opt_d = tf.tuple(clipped_var_d)\n",
    "    ########################### End Optimization ################################\n",
    "\n",
    "    return opt_g, opt_d, opt_c_f, opt_c_r, real_data, real_label, z_cat, z_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate, optimizer, variables, global_step, summaries):\n",
    "    \"\"\"Modified from sugartensor\"\"\"\n",
    "\n",
    "    optim = optimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Calculate Gradient\n",
    "    gradients = optim.compute_gradients(loss, var_list=variables)\n",
    "\n",
    "    # Add Summary\n",
    "    if summaries is None:\n",
    "        summaries = [\"loss\", \"learning_rate\"]\n",
    "    if \"gradient_norm\" in summaries:\n",
    "        tf.summary.scalar(\"global_norm/gradient_norm\",\n",
    "                          clip_ops.global_norm(list(zip(*gradients))[0]))\n",
    "    # Add scalar summary for loss.\n",
    "    if \"loss\" in summaries:\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Add histograms for variables, gradients and gradient norms.\n",
    "    for gradient, variable in gradients:\n",
    "        if isinstance(gradient, ops.IndexedSlices):\n",
    "            grad_values = gradient.values\n",
    "        else:\n",
    "            grad_values = gradients\n",
    "\n",
    "        if grad_values is not None:\n",
    "            var_name = variable.name.replace(\":\", \"_\")\n",
    "            if \"gradients\" in summaries:\n",
    "                tf.summary.histogram(\"gradients/%s\" % var_name, grad_values)\n",
    "            if \"gradient_norm\" in summaries:\n",
    "                tf.summary.scalar(\"gradient_norm/%s\" % var_name,\n",
    "                                  clip_ops.global_norm([grad_values]))\n",
    "\n",
    "    # Gradient Update OP\n",
    "    return optim.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "batch_size = 64\n",
    "img_dim = 32\n",
    "z_dim = 128\n",
    "y_dim = 10\n",
    "learning_rate_ger = 2e-4\n",
    "learning_rate_dis = 2e-4\n",
    "learning_rate_cat = 2e-4\n",
    "device = '/gpu:0'\n",
    "data_format = 'NCHW'\n",
    "# img size\n",
    "s = 32\n",
    "# update Diters times of critic in one iter(unless i < 25 or i % 500 == 0, i is iterstep)\n",
    "c_interv = 2.0\n",
    "Diters = 5\n",
    "Diters = int((c_interv + 1) / c_interv * Diters)\n",
    "# the upper bound and lower bound of parameters in critic\n",
    "clamp_lower = -0.01\n",
    "clamp_upper = 0.01\n",
    "# whether to use adam for parameter update, if the flag is set False, use tf.train.RMSPropOptimizer\n",
    "# as recommended in paper\n",
    "is_adam = False\n",
    "channel = 1\n",
    "# directory to store log, including loss and grad_norm of generator and critic\n",
    "log_dir = './log_wgan'\n",
    "ckpt_dir = './ckpt_wgan'\n",
    "classifier_dir = './cat_wgan'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "if not os.path.exists(classifier_dir):\n",
    "    os.makedirs(classifier_dir)\n",
    "# max iter step, note the one step indicates that a Citers updates of critic and one update of generator\n",
    "max_iter_step = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    with tf.device(device):\n",
    "        opt_g, opt_d, opt_c_f, opt_c_r, real_data, real_label, z_cat, z_rand = build_graph()\n",
    "\n",
    "    merged_all = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "    def next_feed_dict(iter):\n",
    "        train_img, train_label = dataset.train.next_batch(batch_size)\n",
    "        train_img = 2 * train_img - 1\n",
    "\n",
    "        train_img = np.reshape(train_img, (-1, channel, 28, 28))\n",
    "        npad = ((0, 0), (0, 0), (2, 2), (2, 2))\n",
    "        train_img = np.pad(train_img, pad_width=npad,\n",
    "                           mode='constant', constant_values=-1)\n",
    "        batch_z_rand = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "        # Generate random one-hot vectors as class condition\n",
    "        if iter % 2 == 0:\n",
    "            idx = np.random.random_integers(0, y_dim - 1, size=(batch_size,))\n",
    "        else:\n",
    "            idx = np.random.random_integers(0, y_dim - 1)\n",
    "        y_generated = np.zeros((batch_size, y_dim))\n",
    "        y_generated[np.arange(batch_size), idx] = 1\n",
    "\n",
    "        feed_dict = {real_data: train_img, real_label: train_label, z_cat: y_generated, z_rand: batch_z_rand}\n",
    "        return feed_dict\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        for i in range(max_iter_step):\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "\n",
    "            if i < 25 or i % 500 == 0:\n",
    "                diters = 100\n",
    "            else:\n",
    "                diters = Diters * 2\n",
    "\n",
    "            # Train Discriminator\n",
    "            for j in range(diters):\n",
    "                feed_dict = next_feed_dict(i)\n",
    "                if i % 100 == 99 and j == 0:\n",
    "                    run_options = tf.RunOptions(\n",
    "                        trace_level=tf.RunOptions.NO_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    _, merged = sess.run([opt_d, merged_all], feed_dict=feed_dict,\n",
    "                                         options=run_options, run_metadata=run_metadata)\n",
    "                    summary_writer.add_summary(merged, i)\n",
    "                    summary_writer.add_run_metadata(\n",
    "                        run_metadata, 'discriminator_metadata {}'.format(i), i)\n",
    "                else:\n",
    "                    sess.run(opt_d, feed_dict=feed_dict)\n",
    "\n",
    "            # Train Generator\n",
    "            feed_dict = next_feed_dict(i)\n",
    "            if i % 100 == 99:\n",
    "                _, merged = sess.run([opt_g, merged_all], feed_dict=feed_dict,\n",
    "                                     options=run_options, run_metadata=run_metadata)\n",
    "                summary_writer.add_summary(merged, i)\n",
    "                summary_writer.add_run_metadata(\n",
    "                    run_metadata, 'generator_metadata {}'.format(i), i)\n",
    "            else:\n",
    "                sess.run(opt_g, feed_dict=feed_dict)\n",
    "\n",
    "            # Train Generator on classifier\n",
    "            if i % c_interv == 0:\n",
    "                feed_dict = next_feed_dict(i)\n",
    "                if i % 100 == 99:\n",
    "                    _, merged = sess.run([opt_c_f, merged_all], feed_dict=feed_dict,\n",
    "                                         options=run_options, run_metadata=run_metadata)\n",
    "                    summary_writer.add_summary(merged, i)\n",
    "                    summary_writer.add_run_metadata(\n",
    "                        run_metadata, 'generator_f_metadata {}'.format(i), i)\n",
    "                else:\n",
    "                    sess.run(opt_c_f, feed_dict=feed_dict)\n",
    "\n",
    "            # Train Classifier on fake and real\n",
    "            if i % c_interv == 0:\n",
    "                feed_dict = next_feed_dict(i)\n",
    "                if i % 100 == 99:\n",
    "                    _, merged = sess.run([opt_c_r, merged_all], feed_dict=feed_dict,\n",
    "                                         options=run_options, run_metadata=run_metadata)\n",
    "                    summary_writer.add_summary(merged, i)\n",
    "                    summary_writer.add_run_metadata(\n",
    "                        run_metadata, 'classifier_r_metadata {}'.format(i), i)\n",
    "                else:\n",
    "                    sess.run(opt_c_r, feed_dict=feed_dict)\n",
    "\n",
    "            # Save model\n",
    "            if i % 1000 == 999:\n",
    "                saver.save(sess, os.path.join(\n",
    "                    ckpt_dir, \"model.ckpt\"), global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(test_num=10):\n",
    "    img = np.zeros((img_dim * test_num, img_dim * test_num, channel))\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(\n",
    "            allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        with tf.device(device):\n",
    "            train, z_cat, z_rand = build_graph(is_test=True)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "\n",
    "        for i in range(10):\n",
    "            batch_z = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "            # Generate one-hot vectors as class condition\n",
    "            y_generated = np.zeros((batch_size, y_dim))\n",
    "            y_generated[:, i] = 1\n",
    "\n",
    "            output = sess.run(tf.transpose(train, (0, 2, 3, 1)), feed_dict={z_rand: batch_z, z_cat: y_generated})\n",
    "\n",
    "            img[:, img_dim*i:img_dim*(i+1), :] = np.reshape(output[:test_num, :, :, :], (test_num * img_dim, img_dim, channel))\n",
    "\n",
    "    image = ((img / 2 + 0.5)*255).astype(np.uint8)\n",
    "    plt.imshow(\"test\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes must be equal rank, but are 5 and 2\n\tFrom merging shape 8 with other shapes. for 'global_norm_1/t_0' (op: 'Pack') with input shapes: [2,138,8192], [2,8192], [2,3,3,256,512], [2,256], [2,3,3,128,256], [2,128], [2,3,3,64,128], [2,64], [2,3,3,1,64], [2,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 5 and 2\n\tFrom merging shape 8 with other shapes. for 'global_norm_1/t_0' (op: 'Pack') with input shapes: [2,138,8192], [2,8192], [2,3,3,256,512], [2,256], [2,3,3,128,256], [2,128], [2,3,3,64,128], [2,64], [2,3,3,1,64], [2,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2a461e95b42a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mopt_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_c_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_c_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_rand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmerged_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-799db1dcf7a6>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(is_test)\u001b[0m\n\u001b[1;32m     65\u001b[0m                      \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_adam\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                      \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtheta_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcounter_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                      summaries='gradient_norm')\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mcounter_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     opt_d = optimize(loss=d_loss, learning_rate=learning_rate_dis,\n",
      "\u001b[0;32m<ipython-input-7-2b54a90fc479>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(loss, learning_rate, optimizer, variables, global_step, summaries)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"gradient_norm\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msummaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 tf.summary.scalar(\"gradient_norm/%s\" % var_name,\n\u001b[0;32m---> 32\u001b[0;31m                                   clip_ops.global_norm([grad_values]))\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[1;31m# Gradient Update OP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\u001b[0m in \u001b[0;36mglobal_norm\u001b[0;34m(t_list, name)\u001b[0m\n\u001b[1;32m    138\u001b[0m             name=\"t_%d\" % i)\n\u001b[1;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         for i, t in enumerate(t_list)]\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mhalf_squared_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m             name=\"t_%d\" % i)\n\u001b[1;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         for i, t in enumerate(t_list)]\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mhalf_squared_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    903\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"packed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[1;31m# pylint: enable=invalid-name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m    866\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m    867\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m--> 868\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_pack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1887\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mpacked\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \"\"\"\n\u001b[0;32m-> 1889\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pack\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes must be equal rank, but are 5 and 2\n\tFrom merging shape 8 with other shapes. for 'global_norm_1/t_0' (op: 'Pack') with input shapes: [2,138,8192], [2,8192], [2,3,3,256,512], [2,256], [2,3,3,128,256], [2,128], [2,3,3,64,128], [2,64], [2,3,3,1,64], [2,1]."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
